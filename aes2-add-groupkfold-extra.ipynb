{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47020488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:11:16.175683Z",
     "iopub.status.busy": "2024-06-02T15:11:16.174956Z",
     "iopub.status.idle": "2024-06-02T15:15:57.474652Z",
     "shell.execute_reply": "2024-06-02T15:15:57.473532Z"
    },
    "papermill": {
     "duration": 281.312397,
     "end_time": "2024-06-02T15:15:57.477292",
     "exception": false,
     "start_time": "2024-06-02T15:11:16.164895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import aes2_added_fb_prize_as_features_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5578158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:15:57.495608Z",
     "iopub.status.busy": "2024-06-02T15:15:57.495258Z",
     "iopub.status.idle": "2024-06-02T15:15:57.995366Z",
     "shell.execute_reply": "2024-06-02T15:15:57.994322Z"
    },
    "papermill": {
     "duration": 0.511276,
     "end_time": "2024-06-02T15:15:57.997389",
     "exception": false,
     "start_time": "2024-06-02T15:15:57.486113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a43378",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-02T15:15:58.015237Z",
     "iopub.status.busy": "2024-06-02T15:15:58.014950Z",
     "iopub.status.idle": "2024-06-02T15:16:37.948279Z",
     "shell.execute_reply": "2024-06-02T15:16:37.947451Z"
    },
    "papermill": {
     "duration": 39.944833,
     "end_time": "2024-06-02T15:16:37.950628",
     "exception": false,
     "start_time": "2024-06-02T15:15:58.005795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append('/home/mcq/GitHub/aes2')\n",
    "sys.path.append('/home/mcq/GitHub/aes2/topic-classificatio')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pandas as pd\n",
    "import pickle\n",
    "with open(\"/home/mcq/GitHub/aes2/train_data/train_feats.pkl\", \"rb\") as f:\n",
    "    train_feats = pickle.load(f)\n",
    "with open(\"/home/mcq/GitHub/aes2/train_data/X.pkl\", \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "with open(\"/home/mcq/GitHub/aes2/train_data/y.pkl\", \"rb\") as f:\n",
    "    y = pickle.load(f)\n",
    "with open(\"/home/mcq/GitHub/aes2/train_data/y_split.pkl\", \"rb\") as f:\n",
    "    y_split = pickle.load(f)\n",
    "with open(\n",
    "    \"/home/mcq/GitHub/aes2/train_data/feature_select.pkl\", \"rb\"\n",
    ") as f:\n",
    "    feature_select = pickle.load(f)\n",
    "with open(\n",
    "    \"/home/mcq/GitHub/aes2/train_data/base_skf_feat/category-feat.pkl\", \"rb\"\n",
    ") as f:\n",
    "    topic = pickle.load(f)\n",
    "    \n",
    "aes2_added_fb_prize_as_features_preprocessing.feature_select = feature_select\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c42bf6cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:16:37.969708Z",
     "iopub.status.busy": "2024-06-02T15:16:37.969002Z",
     "iopub.status.idle": "2024-06-02T15:16:37.995930Z",
     "shell.execute_reply": "2024-06-02T15:16:37.995034Z"
    },
    "papermill": {
     "duration": 0.03848,
     "end_time": "2024-06-02T15:16:37.997882",
     "exception": false,
     "start_time": "2024-06-02T15:16:37.959402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "train_feats.iloc[:5, -20:]\n",
    "train_feats_1 = train_feats[train_feats['essay_id'].str.len() == 12] #筛选额外数据集的feats\n",
    "train_feats_2 = train_feats[train_feats['essay_id'].str.len() == 7] #筛选原数据集的feats\n",
    "print(len(train_feats_1.shape))\n",
    "print(len(train_feats_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6222a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30178, 13000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f9552a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:16:38.016501Z",
     "iopub.status.busy": "2024-06-02T15:16:38.015806Z",
     "iopub.status.idle": "2024-06-02T15:16:39.220138Z",
     "shell.execute_reply": "2024-06-02T15:16:39.219348Z"
    },
    "papermill": {
     "duration": 1.216012,
     "end_time": "2024-06-02T15:16:39.222452",
     "exception": false,
     "start_time": "2024-06-02T15:16:38.006440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clean_feature_names(features):\n",
    "    illegal_chars = ['[', ']', '<', '>']\n",
    "    cleaned_features = []\n",
    "    for feature in features:\n",
    "        for char in illegal_chars:\n",
    "            feature = feature.replace(char, 'lessthan')\n",
    "        cleaned_features.append(feature)\n",
    "    return cleaned_features\n",
    "\n",
    "train_feats.columns = clean_feature_names(train_feats.columns)\n",
    "feature_select = clean_feature_names(feature_select)\n",
    "with open(\"/home/mcq/GitHub/aes2/train_data/train_2.pkl\", \"rb\") as f:\n",
    "    train_2 = pickle.load(f)\n",
    "\n",
    "with open(\"/home/mcq/GitHub/aes2/train_data/GKF_train.pkl\", \"rb\") as f:\n",
    "    GKF_train = pickle.load(f)\n",
    "\n",
    "X = train_feats[feature_select].astype(np.float32).values\n",
    "y_split = train_2['score'].astype(np.float32).values \n",
    "\n",
    "X_2 = train_feats_1[feature_select].astype(np.float32).values ##注意这里的X2要更改为GKF_train 的feature select 之后的文件\n",
    "#X_2 = train_feats[feature_names].astype(np.float32).values \n",
    "y_split_2 = GKF_train['score'].astype(int).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbf452",
   "metadata": {},
   "source": [
    "# Find Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c46e4afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:16:39.241641Z",
     "iopub.status.busy": "2024-06-02T15:16:39.240836Z",
     "iopub.status.idle": "2024-06-02T15:51:16.585126Z",
     "shell.execute_reply": "2024-06-02T15:51:16.584218Z"
    },
    "papermill": {
     "duration": 2077.356644,
     "end_time": "2024-06-02T15:51:16.587820",
     "exception": false,
     "start_time": "2024-06-02T15:16:39.231176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [12819, 13121]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m train_index_2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     65\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m skf\u001b[38;5;241m.\u001b[39msplit(X_2,y_split_2):\n\u001b[1;32m     67\u001b[0m     train_index_2\u001b[38;5;241m.\u001b[39mappend(train_index)\n\u001b[1;32m     73\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:406\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m        The testing set indices for that split.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:517\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    516\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 517\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:460\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    458\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    463\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [12819, 13121]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, cohen_kappa_score\n",
    "from aes2_added_fb_prize_as_features_preprocessing import *\n",
    "\n",
    "\n",
    "\n",
    "def find_thresholds(true, pred, steps=50):\n",
    "\n",
    "    # SAVE TRIALS FOR PLOTTING\n",
    "    xs = [[],[],[],[],[]]\n",
    "    ys = [[],[],[],[],[]]\n",
    "\n",
    "    # COMPUTE BASELINE METRIC\n",
    "    threshold = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n",
    "                    labels=[1,2,3,4,5,6]).astype('int32')\n",
    "    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n",
    "\n",
    "    # FIND FIVE OPTIMAL THRESHOLDS\n",
    "    for k in range(5):\n",
    "        for sign in [1,-1]:\n",
    "            v = threshold[k]\n",
    "            threshold2 = threshold.copy()\n",
    "            stop = 0\n",
    "            while stop<steps:\n",
    "\n",
    "                # TRY NEW THRESHOLD\n",
    "                v += sign * 0.01\n",
    "                threshold2[k] = v\n",
    "                pred2 = pd.cut(pred, [-np.inf] + threshold2 + [np.inf], \n",
    "                                labels=[1,2,3,4,5,6]).astype('int32')\n",
    "                metric = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n",
    "\n",
    "                # SAVE TRIALS FOR PLOTTING\n",
    "                xs[k].append(v)\n",
    "                ys[k].append(metric)\n",
    "\n",
    "                # EARLY STOPPING\n",
    "                if metric<=best:\n",
    "                    stop += 1\n",
    "                else:\n",
    "                    stop = 0\n",
    "                    best = metric\n",
    "                    threshold = threshold2.copy()\n",
    "\n",
    "    # COMPUTE FINAL METRIC\n",
    "    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n",
    "                    labels=[1,2,3,4,5,6]).astype('int32')\n",
    "    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")   \n",
    "\n",
    "    # RETURN RESULTS\n",
    "    threshold = [np.round(t,3) for t in threshold]\n",
    "    return best, threshold, xs, ys\n",
    "\n",
    "# best, thresholds, xs, ys = find_thresholds(y_split, oof, steps=500)\n",
    "# print('Best thresholds are:', thresholds )\n",
    "# print('=> achieve Overall CV QWK score =', best )\n",
    "\n",
    "# >> Output: thresholds = [1.5, 2.6, 3.5, 4.59, 5.55]\n",
    "# thresholds = [1.51, 2.6, 3.5, 4.59, 5.56]\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=0)\n",
    "# train_index_2 = []\n",
    "# i=1\n",
    "# for train_index, test_index in skf.split(X_2,y_split_2):\n",
    "#     train_index_2.append(train_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "predictions = []\n",
    "f1_scores = []\n",
    "kappa_scores = []\n",
    "groups = topic['topic']\n",
    "tuner_params = {\n",
    "    \"gpu_id\" : 7,\n",
    "    \"learning_rate_lgb\": 0.05,\n",
    "    \"max_depth_lgb\": 8,\n",
    "    \"num_leaves_lgb\": 10,\n",
    "    \"colsample_bytree_lgb\": 0.3,\n",
    "    \"reg_alpha_lgb\": 0.7,\n",
    "    \"reg_lambda_lgb\": 0.1,\n",
    "    \"n_estimators_lgb\": 700,\n",
    "    \"learning_rate_xgb\": 0.1,\n",
    "    \"max_depth_xgb\": 8,\n",
    "    \"num_leaves_xgb\": 10,\n",
    "    \"colsample_bytree_xgb\": 0.5,\n",
    "    \"reg_alpha_xgb\": 0.1,\n",
    "    \"reg_lambda_xgb\": 0.8,\n",
    "    \"n_estimators_xgb\": 1024,\n",
    "    \"n_splits\": 7\n",
    "}\n",
    "\n",
    "n_splits = tuner_params['n_splits']\n",
    "oof = np.zeros(len(train), dtype='float32')\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, models: list):\n",
    "        self.models = models\n",
    "#         self.xgb_boost_best_iter = models[1].\n",
    "    def predict(self, X):\n",
    "        n_models = len(self.models)\n",
    "        predicted = None\n",
    "        # n = 0.709\n",
    "        n = 0.749\n",
    "        for i, model in enumerate(self.models):\n",
    "            if i == 0:\n",
    "                predicted = n*model.predict(X)\n",
    "            else:\n",
    "                # if not isinstance(X, xgb.DMatrix):\n",
    "                #     X = xgb.DMatrix(X)\n",
    "                predicted += (1-n)*model.predict(X)\n",
    "        return predicted\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "xgboost_best_iters = []\n",
    "light_best_iters = []\n",
    "# thresholds = [1.5, 2.5, 3.5, 4.5, 5.5] # init threshold\n",
    "\n",
    "\n",
    "LOAD = True # re-train\n",
    "if not LOAD:\n",
    "    for i in range(n_splits):\n",
    "        random_indices = np.random.choice(X.shape[0], size=5000, replace=False)\n",
    "        random_test_X = X[random_indices, :]\n",
    "        random_test_y = y_split[random_indices]\n",
    "        lgb_predictor = lgb.Booster(model_file=f'kaggle/input/3models/aes-lgbm/fold_{i+1}.txt')\n",
    "        # cat_model = CatBoostRegressor()\n",
    "        # cat_model.load_model(f'kaggle/input/aes-catboost/fold_{i+1}.cbm')\n",
    "        xgb_regressor = xgb.Booster()\n",
    "        xgb_regressor.load_model(f'kaggle/input/3models/aes-xgboost/fold_{i+1}.bin')\n",
    "\n",
    "        predictor = Predictor([lgb_predictor, xgb_regressor])\n",
    "        \n",
    "        models.append(predictor)\n",
    "        \n",
    "        # # Make predictions on the test data for this fold\n",
    "        #         # Make predictions on the test data for this fold\n",
    "        predictions_fold = predictor.predict(random_test_X)\n",
    "        predictions_fold = predictions_fold + a\n",
    "        predictions_fold = predictions_fold.clip(1, 6).round().astype(int)\n",
    "        \n",
    "        # # Confusion Matrix for each fold\n",
    "        # cm = confusion_matrix(random_test_y, predictions_fold, labels=[x for x in range(1,7)])\n",
    "        # disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "        #                               display_labels=[x for x in range(1,7)])\n",
    "        # disp.plot()\n",
    "        # plt.show()\n",
    "        \n",
    "        # # Calculate and store the F1 score for this fold\n",
    "        f1_fold = f1_score(random_test_y, predictions_fold, average='weighted')\n",
    "        f1_scores.append(f1_fold)\n",
    "\n",
    "        # Calculate and store the Cohen's kappa score for this fold\n",
    "        kappa_fold = cohen_kappa_score(random_test_y, predictions_fold, weights='quadratic')\n",
    "        kappa_scores.append(kappa_fold)\n",
    "\n",
    "        print(f'F1 score across fold: {f1_fold}')\n",
    "        print(f'Cohen kappa score across fold: {kappa_fold}')\n",
    "        gc.collect()\n",
    "else:\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(X, y_split,groups), 1):\n",
    "    # Split the data into training and testing sets for this fold\n",
    "        print('fold',i)\n",
    "        # X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "        # y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n",
    "        \n",
    "        \n",
    "        X_train_fold_1, X_test_fold = X[train_index], X[test_index]\n",
    "        X_train_fold_2 = X_2[train_index_2[i]]\n",
    "        X_train_fold = y_train_fold = np.hstack((X_train_fold_1, X_train_fold_2))\n",
    "        ## 链接2个训练集\n",
    "\n",
    "        y_train_fold_1, y_test_fold, y_test_fold_int = y_split[train_index], y_split[test_index], y_split[test_index]\n",
    "        y_train_fold_2 = y_split_2[train_index_2[i]]\n",
    "        y_train_fold = np.hstack((y_train_fold_1, y_train_fold_2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n",
    "        light = lgb.LGBMRegressor(\n",
    "                objective = qwk_obj,\n",
    "                metrics = 'None',\n",
    "                learning_rate = tuner_params[\"learning_rate_lgb\"],\n",
    "                max_depth = tuner_params[\"max_depth_lgb\"],\n",
    "                num_leaves = tuner_params[\"num_leaves_lgb\"],\n",
    "                colsample_bytree = tuner_params[\"colsample_bytree_lgb\"],\n",
    "                reg_alpha = tuner_params[\"reg_alpha_lgb\"],\n",
    "                reg_lambda = tuner_params[\"reg_lambda_lgb\"],\n",
    "                n_estimators = tuner_params[\"n_estimators_lgb\"],\n",
    "                random_state=42,\n",
    "                extra_trees=True,\n",
    "                class_weight='balanced',\n",
    "                # device='gpu' if CUDA_AVAILABLE else 'cpu',\n",
    "                # device_type=\"gpu\",\n",
    "                verbosity = - 1\n",
    "            )\n",
    "\n",
    "        # Fit the model on the training data for this fold  \n",
    "        light.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            eval_names=['train', 'valid'],\n",
    "            eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n",
    "            eval_metric=quadratic_weighted_kappa,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        light_best_iters.append(light.best_iteration_)\n",
    "        \n",
    "        \n",
    "        light.booster_.save_model(f'kaggle/out/aes-lgbm/fold_{i}.txt')\n",
    "        print('\\nFold_{} LightGBM Model saved.\\n'.format(i))\n",
    "        \n",
    "        \n",
    "        xgb_callbacks = [\n",
    "            xgb.callback.EvaluationMonitor(period=25),\n",
    "            xgb.callback.EarlyStopping(75, metric_name=\"QWK\", maximize=True, save_best=True)\n",
    "        ]\n",
    "        xgb_regressor = xgb.XGBRegressor(\n",
    "            objective = qwk_obj,\n",
    "            metrics = 'None',\n",
    "            learning_rate = tuner_params[\"learning_rate_xgb\"],\n",
    "            max_depth = tuner_params[\"max_depth_xgb\"],\n",
    "            num_leaves = tuner_params[\"num_leaves_xgb\"],\n",
    "            colsample_bytree = tuner_params[\"colsample_bytree_xgb\"],\n",
    "            reg_alpha = tuner_params[\"reg_alpha_xgb\"],\n",
    "            reg_lambda = tuner_params[\"reg_lambda_xgb\"],\n",
    "            n_estimators = tuner_params[\"n_estimators_xgb\"],\n",
    "            random_state=42,\n",
    "            extra_trees=True,\n",
    "            # scale_pos_weight=100,\n",
    "            class_weight='balanced',\n",
    "            tree_method=\"hist\",\n",
    "            # device=\"gpu\" if CUDA_AVAILABLE else \"cpu\",\n",
    "            gpu_id = tuner_params[\"gpu_id\"]\n",
    "        #             device='gpu',\n",
    "        #             verbosity = 1\n",
    "        )\n",
    "        \n",
    "        xgb_callbacks = [\n",
    "            xgb.callback.EvaluationMonitor(period=25),\n",
    "            xgb.callback.EarlyStopping(75, metric_name=\"QWK\", maximize=True, save_best=True)\n",
    "        ]\n",
    "        xgb_regressor.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n",
    "            eval_metric=quadratic_weighted_kappa,\n",
    "            callbacks=xgb_callbacks\n",
    "        )\n",
    "        xgboost_best_iters.append(xgb_regressor.get_booster().best_iteration)\n",
    "        \n",
    "        xgb_regressor.save_model(f'kaggle/out/aes-xgboost/fold_{i}.bin')\n",
    "        print('\\nFold_{} XGBoost Model saved.\\n'.format(i)) \n",
    "        \n",
    "        predictor = Predictor([light, xgb_regressor])\n",
    "        \n",
    "        models.append(predictor)\n",
    "        # Make predictions on the test data for this fold\n",
    "        predictions_fold = predictor.predict(X_test_fold)\n",
    "        predictions_fold = predictions_fold + a\n",
    "        \n",
    "        \n",
    "        # predictions_fold = pd.cut(\n",
    "        #     predictions_fold, [-np.inf] + thresholds + [np.inf], \n",
    "        #     labels=[1,2,3,4,5,6]\n",
    "        #     ).astype('int32')\n",
    "        # oof[test_index] = predictions_fold\n",
    "        \n",
    "        predictions_fold = predictions_fold.clip(1, 6).round()\n",
    "        predictions.append(predictions_fold)\n",
    "        # Calculate and store the F1 score for this fold\n",
    "        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n",
    "        f1_scores.append(f1_fold)\n",
    "\n",
    "        # Calculate and store the Cohen's kappa score for this fold\n",
    "        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n",
    "        kappa_scores.append(kappa_fold)\n",
    "    #         predictor.booster_.save_model(f'fold_{i}.txt')\n",
    "        cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=[x for x in range(1,7)]\n",
    "        )\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "        print(f'F1 score across fold: {f1_fold}')\n",
    "        print(f'Cohen kappa score across fold: {kappa_fold}')\n",
    "\n",
    "        gc.collect()\n",
    "        #if ENABLE_DONT_WASTE_YOUR_RUN_TIME:\n",
    "        #    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e8822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:51:17.312550Z",
     "iopub.status.busy": "2024-06-02T15:51:17.312008Z",
     "iopub.status.idle": "2024-06-02T15:51:17.319001Z",
     "shell.execute_reply": "2024-06-02T15:51:17.317979Z"
    },
    "papermill": {
     "duration": 0.189866,
     "end_time": "2024-06-02T15:51:17.320888",
     "exception": false,
     "start_time": "2024-06-02T15:51:17.131022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score across 7 folds: 0.6581823825289266\n",
      "Mean Cohen kappa score across 7 folds: 0.8039929789959077\n"
     ]
    }
   ],
   "source": [
    "mean_f1_score = np.mean(f1_scores)\n",
    "mean_kappa_score = np.mean(kappa_scores)\n",
    "# Print the mean scores\n",
    "print(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\n",
    "print(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')\n",
    "# print(f\"XGBoost mean best iters: {sum(xgboost_best_iters)/len(xgboost_best_iters)}\")\n",
    "# print(f\"LightBoost mean best iters: {sum(light_best_iters)/len(light_best_iters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76752f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:51:17.683801Z",
     "iopub.status.busy": "2024-06-02T15:51:17.683169Z",
     "iopub.status.idle": "2024-06-02T15:53:37.174280Z",
     "shell.execute_reply": "2024-06-02T15:53:37.173190Z"
    },
    "papermill": {
     "duration": 139.675081,
     "end_time": "2024-06-02T15:53:37.176366",
     "exception": false,
     "start_time": "2024-06-02T15:51:17.501285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.19.1\n",
      "transformers.__version__: 4.41.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a055e77c9d8146f28ae74f54c58c0e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 502.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from aes2_added_fb_prize_as_features_preprocessing import preprocess_test, infer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maes2_added_fb_prize_as_features_preprocessing\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m test_feats \u001b[38;5;241m=\u001b[39m \u001b[43maes2_added_fb_prize_as_features_preprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_feats\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m clean_feature_names(test_feats\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      6\u001b[0m aes2_added_fb_prize_as_features_preprocessing\u001b[38;5;241m.\u001b[39minfer(test_feats, models)\n",
      "File \u001b[0;32m~/GitHub/aes2/aes2_added_fb_prize_as_features_preprocessing.py:707\u001b[0m, in \u001b[0;36mpreprocess_test\u001b[0;34m(test)\u001b[0m\n\u001b[1;32m    697\u001b[0m test_feats \u001b[38;5;241m=\u001b[39m test_feats\u001b[38;5;241m.\u001b[39mmerge(df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124messay_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# HashingVectorizer\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# test_tfid = vectorizer_hash.transform([i for i in test['full_text']])\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# dense_matrix = test_tfid.toarray()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# df['essay_id'] = test_feats['essay_id']\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# test_feats = test_feats.merge(df, on='essay_id', how='left')\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m predicted_score \u001b[38;5;241m=\u001b[39m \u001b[43mget_deberta_predicted_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m    709\u001b[0m     test_feats[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeberta_oof_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_score[:, i]\n",
      "File \u001b[0;32m~/GitHub/aes2/aes2_added_fb_prize_as_features_preprocessing.py:127\u001b[0m, in \u001b[0;36mget_deberta_predicted_score\u001b[0;34m(df_test)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m    126\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m--> 127\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataCollatorWithPadding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     preds \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(ds)\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m    135\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(softmax(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:528\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    527\u001b[0m ):\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:775\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 775\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/modeling_utils.py:2724\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2723\u001b[0m         )\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 502.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "# from aes2_added_fb_prize_as_features_preprocessing import preprocess_test, infer\n",
    "import aes2_added_fb_prize_as_features_preprocessing\n",
    "\n",
    "test_feats = aes2_added_fb_prize_as_features_preprocessing.preprocess_test()\n",
    "test_feats.columns = clean_feature_names(test_feats.columns)\n",
    "aes2_added_fb_prize_as_features_preprocessing.infer(test_feats, models)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 2453564,
     "sourceId": 4155787,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2508107,
     "sourceId": 4256323,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4813598,
     "sourceId": 8141507,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4832208,
     "sourceId": 8166166,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4791897,
     "sourceId": 8339744,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 175940118,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 176861104,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 176989302,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 177525613,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 177724159,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2546.990121,
   "end_time": "2024-06-02T15:53:40.489257",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-02T15:11:13.499136",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0cdeae3c30144b148c576c847f965f7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d98b00034a2247afb877762c5e01b971",
       "placeholder": "​",
       "style": "IPY_MODEL_b55bbd361f6a4d6ab300cbfdbda2ecd4",
       "value": "Map: 100%"
      }
     },
     "194804008ff54a70b0a97f3db8fdecbd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2786df8b0f0840b5a17f6a974f607b0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c4d0a5596a04c2482cde8ded99c1363": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34cb97d668d54707b602e01f92b66bfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0cdeae3c30144b148c576c847f965f7e",
        "IPY_MODEL_aeaae7a06c3440cca7fef5beb58b816e",
        "IPY_MODEL_4141ce0201194c7bafe15889cfbd9a4f"
       ],
       "layout": "IPY_MODEL_aa27cd84b72740119a5cd85cf36ebcad"
      }
     },
     "351702fba9b04640928e6389ffaaa193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "35345b3f7de44103b2e7b1b9124322ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3a65b2c90d2944cdb0491a545b740e9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c4d0a5596a04c2482cde8ded99c1363",
       "placeholder": "​",
       "style": "IPY_MODEL_351702fba9b04640928e6389ffaaa193",
       "value": " 1/1 [00:01&lt;00:00,  1.16s/it]"
      }
     },
     "4141ce0201194c7bafe15889cfbd9a4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a34ccd90b88842869298125923204da2",
       "placeholder": "​",
       "style": "IPY_MODEL_8a38608b91ee41aeba3cb53cf4b93a9c",
       "value": " 3/3 [00:00&lt;00:00, 65.17 examples/s]"
      }
     },
     "600e1016d78749fab150b3cba337c6fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8a38608b91ee41aeba3cb53cf4b93a9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8e3aaf6bc26b4b678804acd1e268bc10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "94c0adf705c74af8889871d9bb88da55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a34ccd90b88842869298125923204da2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa27cd84b72740119a5cd85cf36ebcad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aeaae7a06c3440cca7fef5beb58b816e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f16392a4bedd40589fd4221f8c55f11a",
       "max": 3,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_600e1016d78749fab150b3cba337c6fa",
       "value": 3
      }
     },
     "b55bbd361f6a4d6ab300cbfdbda2ecd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b672493ce1d7438a99da0c6394d1ca0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2786df8b0f0840b5a17f6a974f607b0e",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8e3aaf6bc26b4b678804acd1e268bc10",
       "value": 1
      }
     },
     "d98b00034a2247afb877762c5e01b971": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dbf6054374134d768decc541fb5a04a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fe950becf9374c4d8dce62bc2cf1e745",
        "IPY_MODEL_b672493ce1d7438a99da0c6394d1ca0e",
        "IPY_MODEL_3a65b2c90d2944cdb0491a545b740e9d"
       ],
       "layout": "IPY_MODEL_94c0adf705c74af8889871d9bb88da55"
      }
     },
     "f16392a4bedd40589fd4221f8c55f11a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe950becf9374c4d8dce62bc2cf1e745": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_194804008ff54a70b0a97f3db8fdecbd",
       "placeholder": "​",
       "style": "IPY_MODEL_35345b3f7de44103b2e7b1b9124322ec",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
