{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集，并且转变prompt 为 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"  # 指定使用 GPU 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 01:04:55.741598: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-30 01:04:55.803903: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-30 01:04:55.803947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-30 01:04:55.805436: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-30 01:04:55.814763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-30 01:04:57.447280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/mcq/GitHub/aes2/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(PATH + \"train.csv\")\n",
    "#data1 = pd.read_csv(PATHS.train_path)\n",
    "persuade = pd.read_csv('/home/mcq/GitHub/aes2/train_data/persuade_2.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将prompt转变为数字\n",
    "persuade[\"label\"] =  persuade[\"prompt_name\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算交叉点\n",
    "intersection = pd.merge(train_df, persuade, on=\"full_text\", how=\"inner\")[[\"essay_id\", \"full_text\", \"score\", \"prompt_name\",\"label\"]].reset_index(drop=True)\n",
    "persuade.rename(columns={'essay_id_comp': 'essay_id'}, inplace=True)\n",
    "persuade.rename(columns={'holistic_essay_score': 'score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集去除交叉数据， 返回大数据集labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(intersection): 4436\n",
      "len(difference): 25996\n"
     ]
    }
   ],
   "source": [
    "difference = train_df[~train_df[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "###直接替换SKF的train就可以 \n",
    "difference1 = persuade[~persuade[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\n",
    "difference1 = difference1.iloc[:, :3] \n",
    "skf_train = pd.concat([train_df, difference1], axis=0, ignore_index=True) \n",
    "### skf_train就是加过额外数据集的train了， \n",
    "\n",
    "\n",
    "\n",
    "difference2 = persuade[~persuade[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\n",
    "# print(\"len(intersection):\", len(difference))\n",
    "# print(\"len(difference):\", len(difference2))\n",
    "### 有25996个数据集\n",
    "###首先drop掉0-6之间的topic数据点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = difference2['label'].isin([1,  5 , 0 , 7 , 6 , 8 ,14])\n",
    "df_dropped = difference2[~mask]\n",
    "GFX_add7 = difference2[~difference2[\"essay_id\"].isin(df_dropped[\"essay_id\"])].reset_index(drop=True)\n",
    "##GFX_add7长度为12873，是在7个topic之内的点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BC75783F96E3</td>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74C8BC7417DE</td>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A8445CABFECE</td>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6B4F7A0165B9</td>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id                                          full_text  score\n",
       "0  423A1CA112E2  Phones\\n\\nModern humans today are always on th...      3\n",
       "1  BC75783F96E3  This essay will explain if drivers should or s...      4\n",
       "2  74C8BC7417DE  Driving while the use of cellular devices\\n\\nT...      2\n",
       "3  A8445CABFECE  Phones & Driving\\n\\nDrivers should not be able...      3\n",
       "4  6B4F7A0165B9  Cell Phone Operation While Driving\\n\\nThe abil...      4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GKF_train = df_dropped ###长度为13121,是topic之外的点\n",
    "GKF_train = GKF_train.iloc[:, :3]  ##数据规范化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在原始数据集中添加额外的7topics之内的代码，对应的是GFX_add7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7612dea7dc6d4ed9bbd33265c80a44fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcq2/anaconda3/envs/aes/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import topicfeaturesmall\n",
    "PATH = \"/home/mcq2/GitHub/aes2/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n",
    "train = pd.read_csv(PATH + \"train.csv\")\n",
    "topic = topicfeaturesmall.predict_chunk(train)\n",
    "groups_1= topic['topic'] ## 读取一万七的topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_2 = GFX_add7['label'] ##读取额外的topic\n",
    "groups = pd.concat([groups_1, groups_2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFX_add7 = GFX_add7.iloc[:, :3] ### 注意这个一定要放在group提取之后，因为这里已经把topic列删掉了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = pd.concat([train, GFX_add7], axis=0, ignore_index=True) ###把额外的7个topic之内的代码整合进原有的数据集，这里的代码结构和train一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上面的train2 用来代替train跑feature和feature_selection的代码！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的代码就是分组，一个在GKF里面，一个在GKF外面，放在baseline代码的GKF就可以了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = train_feats[feature_names].astype(np.float32).values \n",
    "\n",
    "X = train_2 # ### 在这里的时候使用train_2跑feature_selection之后的pickel文件\n",
    "y_split = train_2['score'].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GKF代码, 只加入了一万二的topic=7的额外数据集\n",
    "###这个代码可以与下面的进行整合\n",
    "### 写在这里是为了让你参考它写SKF的代码的，\n",
    "### 所以我注释掉了\n",
    "# from sklearn.model_selection import GroupKFold\n",
    "# LOAD = True \n",
    "\n",
    "# n_splits = 7 ## 如果变更为大数据集，这个_split要更改\n",
    "# models = []\n",
    "\n",
    "\n",
    "# if not LOAD:\n",
    "#     for i in range(n_splits):\n",
    "#         models.append(lgb.Booster(model_file=f'kaggle/input/aes-lgbm/fold_{i+1}.txt'))\n",
    "# else:\n",
    "#     group_kfold = GroupKFold(n_splits=n_splits)\n",
    "#     f1_scores = []\n",
    "#     kappa_scores = []\n",
    "#     models = []\n",
    "#     predictions = []\n",
    " \n",
    "#     for i, (train_index, test_index) in enumerate(group_kfold.split(X, y_split, groups)):\n",
    "#         X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "#         y_train_fold, y_test_fold, y_test_fold_int = y_split[train_index], y_split[test_index], y_split[test_index]\n",
    "#         # 可以通过这三行代码看一下数据，直接删掉就可以\n",
    "#         # print(f\"Fold {i}:\")\n",
    "#         # print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
    "#         # print(f\"  Test:  index={test_index}, group={groups[test_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在for循环里加入额外的训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = GKF_train ##注意这里的X2要更改为GKF_train 的feature select 之后的文件\n",
    "#X_2 = train_feats[feature_names].astype(np.float32).values \n",
    "y_split_2 = GKF_train['score'].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=0)\n",
    "train_index_2 = []\n",
    "i=1\n",
    "for train_index, test_index in skf.split(X_2,y_split_2):\n",
    "    train_index_2.append(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "LOAD = True \n",
    "\n",
    "n_splits = 7 ## 如果变更为大数据集，这个_split要更改\n",
    "models = []\n",
    "\n",
    "\n",
    "if not LOAD:\n",
    "    for i in range(n_splits):\n",
    "        models.append(lgb.Booster(model_file=f'kaggle/input/aes-lgbm/fold_{i+1}.txt'))\n",
    "else:\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    f1_scores = []\n",
    "    kappa_scores = []\n",
    "    models = []\n",
    "    predictions = []\n",
    " \n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(X, y_split, groups)):\n",
    "        ### X, y_split是已经加入额外数据集的代码\n",
    "        ###你做SKF也只要之前修改数据集就完事了\n",
    "        X_train_fold_1, X_test_fold = X[train_index], X[test_index]\n",
    "        X_train_fold_2 = X_2[train_index_2[i]]\n",
    "        X_train_fold = y_train_fold = np.hstack((X_train_fold_1, X_train_fold_2))\n",
    "        ## 链接2个训练集\n",
    "\n",
    "        y_train_fold_1, y_test_fold, y_test_fold_int = y_split[train_index], y_split[test_index], y_split[test_index]\n",
    "        y_train_fold_2 = y_split_2[train_index_2[i]]\n",
    "        y_train_fold = np.hstack((y_train_fold_1, y_train_fold_2))\n",
    "        # 可以通过这三行代码看一下数据，直接删掉就可以\n",
    "        # print(f\"Fold {i}:\")\n",
    "        # print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
    "        # print(f\"  Test:  index={test_index}, group={groups[test_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计单词数量\n",
    "train_df['word_count_pure'] = train_df['full_text'].apply(lambda x: len(x.split()))\n",
    "persuade['word_count_pure'] = train_df['full_text'].apply(lambda x: len(x.split()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
