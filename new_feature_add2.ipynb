{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature augmentation : Universal Sentence Encoder¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import polars as pl\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n",
    "train = pd.read_csv(PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新加特征1：句子，单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features engineering\n",
    "#Preprocessing\n",
    "\n",
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?\\n>')\n",
    "    return html.sub(r'',x)\n",
    "\n",
    "def dataPreprocessing(x):\n",
    "    # lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    # Delete aposhtroph html\n",
    "    #x = re.sub(r\"\\\\'\", \"'\", x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "# Paragraph preprocessing\n",
    "train['paragraph_processed'] = [dataPreprocessing(x) for x in train['full_text']]\n",
    "\n",
    "# Calculate total number of sentences\n",
    "train['sentence_cnt'] = [len(x.split('.')) for x in train['paragraph_processed']]\n",
    "\n",
    "# Calculate total number of words\n",
    "train['word_cnt'] = [len(x.split(' ')) for x in train['paragraph_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Calculate statistical parameters of paragraphs, sentences, and words\n",
    "import statistics\n",
    "\n",
    "\n",
    "def corpus_satistics(data, col, heading_len, split_str, corp_unit):\n",
    "    corp_unit_len_min = []\n",
    "    corp_unit_len_max = []\n",
    "    corp_unit_len_mean = []\n",
    "    corp_unit_len_median = []\n",
    "    corp_unit_len_sd = []\n",
    "    corp_unit_len_quantiles =[]\n",
    "    \n",
    "    for z in data[col]:\n",
    "        corpLen_cnt = []\n",
    "        for y in z.split(split_str):\n",
    "            if corp_unit=='word':\n",
    "                x=len(y.split(' '))\n",
    "                if x>3: # Paragraph heading should be limited to 3 words\n",
    "                    corpLen_cnt.append(x)\n",
    "            else:\n",
    "                if len(y)>heading_len: # Paragraph heading should be limited to 15-20 characters\n",
    "                    corpLen_cnt.append(len(y))\n",
    "\n",
    "        corp_unit_len_min.append(min(corpLen_cnt))\n",
    "        corp_unit_len_max.append(max(corpLen_cnt))\n",
    "        corp_unit_len_mean.append(statistics.mean(corpLen_cnt))\n",
    "        corp_unit_len_median.append(statistics.median(corpLen_cnt))\n",
    "        if len(corpLen_cnt)>=2: # As some full_texts have just one paragraph\n",
    "            corp_unit_len_sd.append(statistics.stdev(corpLen_cnt))\n",
    "            qua = statistics.quantiles(corpLen_cnt, n=10, method='exclusive')\n",
    "            qua = [0 if i < 0 else i for i in qua]\n",
    "            corp_unit_len_quantiles.append(qua)\n",
    "        else:\n",
    "            corp_unit_len_sd.append(corpLen_cnt[0]) # sd for single paragraph/sentence entries are kept as large \n",
    "            corp_unit_len_quantiles.append([0]*9) # quantiles for single paragraph/sentence entries are kept zero\n",
    "\n",
    "\n",
    "\n",
    "    data[corp_unit + '_len_min'] = corp_unit_len_min\n",
    "    data[corp_unit + '_len_max'] = corp_unit_len_max\n",
    "    data[corp_unit + '_len_mean'] = corp_unit_len_mean\n",
    "    data[corp_unit + '_len_median'] = corp_unit_len_median\n",
    "    data[corp_unit + '_len_sd'] = corp_unit_len_sd\n",
    "    data[corp_unit + '_len_qua0'] = [x[0] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua1'] = [x[1] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua2'] = [x[2] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua3'] = [x[3] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua4'] = [x[4] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua5'] = [x[5] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua6'] = [x[6] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua7'] = [x[7] for x in corp_unit_len_quantiles]\n",
    "    data[corp_unit + '_len_qua8'] = [x[8] for x in corp_unit_len_quantiles]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Statistics for paragraph\n",
    "\n",
    "data = train\n",
    "col = 'full_text'\n",
    "heading_len = 20\n",
    "split_str = '\\n\\n'\n",
    "corp_unit = 'paragraph'\n",
    "\n",
    "train = corpus_satistics(data, col, heading_len, split_str, corp_unit)\n",
    "\n",
    "# Statistics for sentence\n",
    "\n",
    "data = train\n",
    "col = 'paragraph_processed'\n",
    "heading_len = 15\n",
    "split_str = '.'\n",
    "corp_unit = 'sentence'\n",
    "\n",
    "train = corpus_satistics(data, col, heading_len, split_str, corp_unit)\n",
    "\n",
    "# Statistics for word\n",
    "\n",
    "data = train\n",
    "col = 'paragraph_processed'\n",
    "#heading_len = 15\n",
    "split_str = '.'\n",
    "corp_unit = 'word'\n",
    "\n",
    "train = corpus_satistics(data, col, heading_len, split_str, corp_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:19:26.010874: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-18 23:19:26.050342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-18 23:19:26.050373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-18 23:19:26.051871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-18 23:19:26.059165: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 23:19:26.836645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-18 23:21:21.756150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:52:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.757444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22272 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:53:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.758324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22272 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:56:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.759158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22272 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:57:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.759995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 22272 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:ce:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.760789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 22272 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:d1:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.761612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 22272 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:d2:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.762403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 22272 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:d5:00.0, compute capability: 8.6\n",
      "2024-06-18 23:21:21.763192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:8 with 22272 MB memory:  -> device: 8, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:d6:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.03133019 -0.06338634 -0.016075   ... -0.0324278  -0.0457574\n",
      "   0.05370456]\n",
      " [ 0.05080861 -0.01652431  0.01573778 ...  0.00976659  0.03170121\n",
      "   0.01788118]], shape=(2, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\")\n",
    "\n",
    "sentence_encoder = hub.KerasLayer(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在这里如果要使用模型的话，先下载模型，并且使用以下代码\n",
    "##https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\n",
    "\n",
    "# sentence_encoder = hub.KerasLayer(\n",
    "#     #\"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "#     '/kaggle/input/universal-sentence-encoder/tensorflow2/universal-sentence-encoder/2'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal sentence encoder function\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "def use_function(corpus, column_name):\n",
    "\n",
    "    sencode_corpus = []\n",
    "    for x in corpus[column_name]:\n",
    "\n",
    "      \n",
    "        if len(x.split('.'))<2:\n",
    "            sencode_essay = [0.]*512\n",
    "\n",
    "        else:\n",
    "            enc_raw = sentence_encoder(x.split('.'))[:-1]\n",
    "            sencode_essay = tf.math.reduce_sum(enc_raw, 0).numpy()/math.sqrt(len(x.split('.')))\n",
    "\n",
    "    \n",
    "\n",
    "        sencode_corpus.append(sencode_essay)\n",
    "               \n",
    "\n",
    "    return sencode_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "corpus = train\n",
    "column_name = 'full_text'\n",
    "sencode_corpus = use_function(corpus, column_name)\n",
    "sencode = pd.DataFrame(sencode_corpus)\n",
    "# rename features\n",
    "sencode_columns = [ f'sencode_{i}' for i in range(len(sencode.columns))]\n",
    "sencode.columns = sencode_columns\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "sencode['essay_id'] = train['essay_id']\n",
    "train = train.merge(sencode, on='essay_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Paragraph preprocessing\n",
    "test['paragraph_processed'] = [dataPreprocessing(x) for x in test['full_text']]\n",
    "# Calculate the number of sentences\n",
    "test['sentence_cnt'] = [len(x.split('.')) for x in test['paragraph_processed']]\n",
    "# Calculate the number of words\n",
    "test['word_cnt'] = [len(x.split(' ')) for x in test['paragraph_processed']]\n",
    "\n",
    "# Statistics for paragraph\n",
    "data = test\n",
    "col = 'full_text'\n",
    "heading_len = 20\n",
    "split_str = '\\n\\n'\n",
    "corp_unit = 'paragraph'\n",
    "\n",
    "test = corpus_satistics(data, col, heading_len, split_str, corp_unit)\n",
    "\n",
    "# Statistics for sentence\n",
    "data = test\n",
    "col = 'paragraph_processed'\n",
    "heading_len = 15\n",
    "split_str = '.'\n",
    "corp_unit = 'sentence'\n",
    "\n",
    "test = corpus_satistics(data, col, heading_len, split_str, corp_unit)\n",
    "\n",
    "# Statistics for word\n",
    "data = test\n",
    "col = 'paragraph_processed'\n",
    "#heading_len = 15\n",
    "split_str = '.'\n",
    "corp_unit = 'word'\n",
    "\n",
    "test = corpus_satistics(data, col, heading_len, split_str, corp_unit)\n",
    "# Tfidf and merge\n",
    "test_tfid = vectorizer.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test['essay_id']\n",
    "test = test.merge(df, on='essay_id', how='left')\n",
    "# universal sentence encoder\n",
    "# test \n",
    "corpus = test\n",
    "column_name = 'full_text'\n",
    "sencode_corpus = use_function(corpus, column_name)\n",
    "sencode = pd.DataFrame(sencode_corpus)\n",
    "# rename features\n",
    "sencode_columns = [ f'sencode_{i}' for i in range(len(sencode.columns))]\n",
    "sencode.columns = sencode_columns\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "sencode['essay_id'] = test['essay_id']\n",
    "test = test.merge(sencode, on='essay_id', how='left')\n",
    "# deberta\n",
    "for i in range(6):\n",
    "    test[f'deberta_oof_{i}'] = predicted_score[:, i]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "prediction = test[['essay_id']].copy()\n",
    "prediction['score'] = 0\n",
    "pred_test = models[0].predict(test[feature_names]) + a\n",
    "for i in range(4):\n",
    "    pred_now = models[i+1].predict(test[feature_names]) + a\n",
    "    pred_test = np.add(pred_test,pred_now)\n",
    "# The final prediction result needs to be divided by 5 because the prediction results of 5 models were added together\n",
    "pred_test = pred_test/5\n",
    "print(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the prediction result to an integer and limit it to a range of 1-6 (score range)\n",
    "pred_test = pred_test.clip(1, 6).round()\n",
    "prediction['score'] = pred_test\n",
    "prediction.to_csv('submission.csv', index=False)\n",
    "prediction.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
