{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from thefuzz import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gradio as gr\n",
    "tokenizer_simcse = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\", force_download=True, resume_download=False)\n",
    "model_simcse = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\", force_download=True, resume_download=False)\n",
    "tokenizer_mpnet = AutoTokenizer.from_pretrained('sentence-transformers/stsb-mpnet-base-v2', force_download=True,resume_download=False)\n",
    "model_mpnet = AutoModel.from_pretrained('sentence-transformers/stsb-mpnet-base-v2',force_download=True, resume_download=False)\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(\n",
    "        -1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def thefuzz(text1, text2):\n",
    "    score = fuzz.token_sort_ratio(text1, text2)\n",
    "    return score/100*2-1\n",
    "\n",
    "\n",
    "def tfidf(text1, text2):\n",
    "    t1_tfidf = vectorizer.fit_transform([text1])\n",
    "    t2_tfidf = vectorizer.transform([text2])\n",
    "    cosine_sim = cosine_similarity(t1_tfidf, t2_tfidf).flatten()[0]\n",
    "    return round(cosine_sim, 3)*2-1\n",
    "\n",
    "\n",
    "def simcse(text1, text2):\n",
    "    texts = [text1, text2]\n",
    "    inputs = tokenizer_simcse(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = model_simcse(\n",
    "            **inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "    cosine_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "    return round(cosine_sim, 3)*2-1\n",
    "\n",
    "\n",
    "def mpnet(text1, text2):\n",
    "    encoded_input = tokenizer_mpnet(\n",
    "        [text1, text2], padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model_mpnet(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(\n",
    "        model_output, encoded_input['attention_mask'])\n",
    "    cosine_sim = 1 - cosine(sentence_embeddings[0], sentence_embeddings[1])\n",
    "    return round(cosine_sim, 3)*2-1\n",
    "\n",
    "\n",
    "def rg(text1, text2):\n",
    "    rouge = Rouge()\n",
    "    rouge_sim = rouge.get_scores(\n",
    "        text1, text2)[0][\"rouge-1\"][\"r\"]\n",
    "\n",
    "    return round(rouge_sim, 3)*2-1\n",
    "\n",
    "\n",
    "def get_scores(text1, text2):\n",
    "    fuzz_out = thefuzz(text1, text2)\n",
    "    tfidf_out = tfidf(text1, text2)\n",
    "    simcse_out = simcse(text1, text2)\n",
    "    mpnet_out = mpnet(text1, text2)\n",
    "    rouge_out = rg(text1, text2)\n",
    "    return simcse_out, mpnet_out, fuzz_out, tfidf_out, rouge_out\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
